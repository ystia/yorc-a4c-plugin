tosca_definitions_version: ${alien4cloud.dsl.version}

template_name: yorc-slurm-types
template_author: Yorc
template_version: ${yorc.slurm.types.version}

description: "Defines resources for the Yorc plugin, slurm configuration."

imports:
  - tosca-normative-types:${tosca.normative.types.version}
  - alien-base-types:${alien4cloud.version}
  - yorc-types:${yorc.types.version}


artifact_types:
  yorc.artifacts.Deployment.SlurmJob:
    description: Slurm Job deployment descriptor
    derived_from: tosca.artifacts.Deployment
  yorc.artifacts.Deployment.SlurmJobBatch:
    description: Slurm Job binary deployment descriptor
    derived_from: yorc.artifacts.Deployment.SlurmJob
  yorc.artifacts.Deployment.SlurmJobImage:
    description: Slurm Job Container image deployment descriptor
    derived_from: yorc.artifacts.Deployment.SlurmJob

data_types:
  yorc.datatypes.slurm.JobOptions:
    derived_from: tosca.datatypes.Root
    properties:
      name:
        type: string
        description: The slurm job name.
        required: false
      tasks:
        description: Number of tasks to run.
        type: integer
        required: false
        default: 1
      nodes:
        description: Number of nodes allocated to the job.
        type: integer
        required: false
        default: 1
      cpus_per_task:
        description: Number of cpus allocated per task.
        type: integer
        required: false
      mem_per_node:
        type: scalar-unit.size
        description: The memory per node required to the job.
        required: false
        constraints:
          - greater_or_equal: 0 KB
      time:
        type: string
        description: >
          Set a limit on the total run time of the job allocation.
          Time formats include "minutes", "minutes:seconds", "hours:minutes:seconds", "days-hours", "days-hours:minutes" and "days-hours:minutes:seconds"
        required: false
      reservation:
        type: string
        description: >
           Allocate resources for the job from the named reservation.
        required: false
      account:
        type: string
        description: >
          Charge resources used by this allocation to specified account. May be mandatory according to configuration.
        required: false
      extra_options:
        type: list
        description: >
         This define all other slurm job options (ex: --mpi=pmi2 or --partition=MyPartition).
        required: false
        entry_schema:
          type: string

  yorc.datatypes.slurm.ExecutionOptions:
    derived_from: tosca.datatypes.Root
    properties:
      command:
        type: string
        description: >
          Allows a job to run a command instead of a batch script if none is provided. In case of container,
          In a container context, the specified command is executed instead of default container scripts.
        required: false
      args:
        type: list
        description: >
          If command is provided, this allows to define arguments passed to the command.
        required: false
        entry_schema:
          type: string
      env_vars:
        type: list
        description: Environment variables to pass to the job execution.
        required: false
        entry_schema:
          type: string


capability_types:
  yorc.capabilities.slurm.Endpoint:
    derived_from: yorc.capabilities.Endpoint.ProvisioningAdmin
    properties:
      # Adds non required credentials
      credentials:
        type: yorc.datatypes.ProvisioningCredential
        description: Credentials used to provision the resource
        required: false

node_types:
  yorc.nodes.slurm.Compute:
    derived_from: yorc.nodes.Compute
    properties:
      gres:
        type: string
        required: false
        description: |
          Specifies a comma delimited list of generic consumable resources. The format of each entry on the list is "name[[:type]:count]". The name is that of the consumable resource. The count is the number of those resources with a default value of 1. The specified resources will be allocated to the job on each node. The available generic consumable resources is configurable by the system administrator. Examples of use include "--gres=gpu:2,mic=1", "--gres=gpu:kepler:2", and "--gres=help".
      constraint:
        type: string
        required: false
        description: |
          Nodes can have features assigned to them by the Slurm administrator. Users can specify which of these features are required by their job using the constraint option. Only nodes having features matching the job constraints will be used to satisfy the request. Multiple constraints may be specified with AND, OR, matching OR, resource counts, etc. (some operators are not supported on all system types).
      partition:
        type: string
        required: false
        description: Slurm partition where the nodes will be deployed
      job_name:
        type: string
        required: false
        description: Specify a name for the job allocation. The specified name will appear along with the job id.
      account:
        type: string
        description: >
           Charge resources used by this allocation to specified account. May be mandatory according to configuration.
        required: false
      reservation:
        type: string
        description: >
           Allocate resources from the named reservation.
        required: false
    capabilities:
      endpoint:
        type: yorc.capabilities.slurm.Endpoint

    attributes:
      cuda_visible_devices:
        type: string
        description: Coma separated list of visibles GPU devices for the compute.
      job_id:
        type: string
        description: The ID of the job allocation.
      partition:
        type: string
        description: Slurm partition where the nodes are deployed.

  yorc.nodes.slurm.Job:
    derived_from: org.alien4cloud.nodes.Job
    properties:
      slurm_options:
        type: yorc.datatypes.slurm.JobOptions
        description: >
          Job properties used for Slurm sbatch execution. See Slurm documentation (https://slurm.schedmd.com/sbatch.html) for more details.
        required: false
      working_directory:
        type: string
        description: Directory where the batch script or command will be executed. Default is home's related user.
        required: false
      execution_options:
        type: yorc.datatypes.slurm.ExecutionOptions
        description: >
          Properties used for the execution itself.
        required: false
      monitoring_time_interval:
        type: string
        description: >
          Time interval duration used for job monitoring as "5s" or "300ms"
          Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
        required: false
      credentials:
        type: tosca.datatypes.Credential
        description: >
           Provide user credentials for connection to slurm client node
        required: false
    attributes:
      job_id:
        type: string
        description: The ID of the job.
    interfaces:
      tosca.interfaces.node.lifecycle.Runnable:
        submit:
          implementation:
            # This is a hack to force Alien to generate this step in workflows it will be overrided in Yorc
            # TODO(loicalbertin) think about use a topology modifier for this to add this step only if a submit operation exists
            file: "resources.yaml"
            type: yorc.artifacts.Deployment.SlurmJob
        run:
          implementation:
            # This is a hack to force Alien to generate this step in workflows it will be overrided in Yorc
            # TODO(loicalbertin) think about use a topology modifier for this to add this step only if a submit operation exists
            file: "resources.yaml"
            type: yorc.artifacts.Deployment.SlurmJob
        cancel:
          implementation:
            # This is a hack to force Alien to generate this step in workflows it will be overrided in Yorc
            # TODO(loicalbertin) think about use a topology modifier for this to add this step only if a submit operation exists
            file: "resources.yaml"
            type: yorc.artifacts.Deployment.SlurmJob

  yorc.nodes.slurm.SingularityJob:
    derived_from: yorc.nodes.slurm.Job
    properties:
      singularity_command_options:
        type: list
        description: >
          Options passed to the "singularity run" or "singularity exec" command. See Singularity documentation (https://www.sylabs.io/docs/) for more details.
        required: false
        entry_schema:
          type: string
      singularity_debug:
        type: boolean
        description: Print all debug and verbose information during singularity execution
        required: false
        default: false