tosca_definitions_version: ${alien4cloud.dsl.version}

template_name: yorc-slurm-types
template_author: Yorc
template_version: ${yorc.slurm.types.version}

description: "Defines resources for the Yorc plugin, slurm configuration."

imports:
  - tosca-normative-types:${tosca.normative.types.version}
  - alien-base-types:${alien4cloud.version}
  - yorc-types:${yorc.types.version}


artifact_types:
  yorc.artifacts.Deployment.SlurmJob:
    description: Slurm Job deployment descriptor
    derived_from: tosca.artifacts.Deployment
  yorc.artifacts.Deployment.SlurmJobBin:
    description: Slurm Job binary deployment descriptor
    derived_from: yorc.artifacts.Deployment.SlurmJob
  yorc.artifacts.Deployment.SlurmJobImage:
    description: Slurm Job Container image deployment descriptor
    derived_from: yorc.artifacts.Deployment.SlurmJob

capability_types:
  yorc.capabilities.slurm.Endpoint:
    derived_from: yorc.capabilities.Endpoint.ProvisioningAdmin
    properties:
      # Adds non required credentials
      credentials:
        type: yorc.datatypes.ProvisioningCredential
        description: Credentials used to provision the resource
        required: false

node_types:
  yorc.nodes.slurm.Compute:
    derived_from: yorc.nodes.Compute
    properties:
      gres:
        type: string
        required: false
        description: |
          Specifies a comma delimited list of generic consumable resources. The format of each entry on the list is "name[[:type]:count]". The name is that of the consumable resource. The count is the number of those resources with a default value of 1. The specified resources will be allocated to the job on each node. The available generic consumable resources is configurable by the system administrator. Examples of use include "--gres=gpu:2,mic=1", "--gres=gpu:kepler:2", and "--gres=help".
      constraint:
        type: string
        required: false
        description: |
          Nodes can have features assigned to them by the Slurm administrator. Users can specify which of these features are required by their job using the constraint option. Only nodes having features matching the job constraints will be used to satisfy the request. Multiple constraints may be specified with AND, OR, matching OR, resource counts, etc. (some operators are not supported on all system types).
      partition:
        type: string
        required: false
        description: Slurm partition where the nodes will be deployed
      job_name:
        type: string
        required: false
        description: Specify a name for the job allocation. The specified name will appear along with the job id.
      account:
        type: string
        description: >
           Charge resources used by this allocation to specified account. May be mandatory according to configuration.
        required: false
      reservation:
        type: string
        description: >
           Allocate resources from the named reservation.
        required: false
    capabilities:
      endpoint:
        type: yorc.capabilities.slurm.Endpoint

    attributes:
      cuda_visible_devices:
        type: string
        description: Coma separated list of visibles GPU devices for the compute.
      job_id:
        type: string
        description: The ID of the job allocation.
      partition:
        type: string
        description: Slurm partition where the nodes are deployed.

  yorc.nodes.slurm.Job:
    derived_from: org.alien4cloud.nodes.Job
    properties:
      name:
        type: string
        description: The slurm job name.
        required: false
      tasks:
        description: Number of tasks to run.
        type: integer
        required: false
        default: 1
      nodes:
        description: Number of nodes allocated to the job.
        type: integer
        required: false
        default: 1
      cpus_per_task:
        description: Number of cpus allocated per task.
        type: integer
        required: false
      mem_per_node:
        type: integer
        description: The memory per node in GB required to the job.
        required: false
      time:
        type: string
        description: >
          Set a limit on the total run time of the job allocation.
          Time formats include "minutes", "minutes:seconds", "hours:minutes:seconds", "days-hours", "days-hours:minutes" and "days-hours:minutes:seconds"
        required: false
      extra_job_options:
        type: list
        description: >
         This define all other slurm job options (ex: --mpi=pmi2 or --partition=MyPartition).
        required: false
        entry_schema:
          type: string
      working_directory:
        type: string
        description: Directory where the batch script or command will be executed. Default is home's related user.
        required: false
      command:
        type: string
        description: >
          Allows a job to run a command instead of a batch script if none is provided.
        required: false
      args:
        type: list
        description: >
          If command is provided, this allows to define arguments passed to the command.
        required: false
        entry_schema:
          type: string
      env_vars:
        type: list
        description: Environment variables to pass to the job execution.
        required: false
        entry_schema:
          type: string
      monitoring_time_interval:
        type: string
        description: >
          Time interval duration used for job monitoring as "5s" or "300ms"
          Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
        required: false
      credentials:
        type: tosca.datatypes.Credential
        description: >
           Provide user credentials for connection to slurm client node
        required: false
      reservation:
        type: string
        description: >
           Allocate resources for the job from the named reservation.
        required: false
      account:
        type: string
        description: >
          Charge resources used by this allocation to specified account. May be mandatory according to configuration.
        required: false
    attributes:
      job_id:
        type: string
        description: The ID of the job.
    interfaces:
      tosca.interfaces.node.lifecycle.Runnable:
        submit:
          implementation:
            # This is a hack to force Alien to generate this step in workflows it will be overrided in Yorc
            # TODO(loicalbertin) think about use a topology modifier for this to add this step only if a submit operation exists
            file: "resources.yaml"
            type: yorc.artifacts.Deployment.SlurmJob
        run:
          implementation:
            # This is a hack to force Alien to generate this step in workflows it will be overrided in Yorc
            # TODO(loicalbertin) think about use a topology modifier for this to add this step only if a submit operation exists
            file: "resources.yaml"
            type: yorc.artifacts.Deployment.SlurmJob
        cancel:
          implementation:
            # This is a hack to force Alien to generate this step in workflows it will be overrided in Yorc
            # TODO(loicalbertin) think about use a topology modifier for this to add this step only if a submit operation exists
            file: "resources.yaml"
            type: yorc.artifacts.Deployment.SlurmJob

  yorc.nodes.slurm.SingularityJob:
    derived_from: yorc.nodes.slurm.Job
    properties:
      command:
        type: string
        description: If this property is filled, the specified command is executed within the container with "singularity exec" instead of default "singularity run".
        required: false
      singularity_command_options:
        type: list
        description: Options passed to the "singularity run" or "singularity exec" command.
        required: false
        entry_schema:
          type: string
      singularity_debug:
        type: boolean
        description: Print all debug and verbose information during singularity execution
        required: false
        default: false