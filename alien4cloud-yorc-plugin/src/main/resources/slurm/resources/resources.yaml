tosca_definitions_version: ${alien4cloud.dsl.version}

template_name: yorc-slurm-types
template_author: Yorc
template_version: ${yorc.slurm.types.version}

description: "Defines resources for the Yorc plugin, slurm configuration."

imports:
  - tosca-normative-types:${tosca.normative.types.version}
  - alien-base-types:${alien4cloud.version}
  - yorc-types:${yorc.types.version}


artifact_types:
  yorc.artifacts.Deployment.SlurmJob:
    description: Slurm Job deployment descriptor
    derived_from: tosca.artifacts.Deployment
  yorc.artifacts.Deployment.SlurmJobBin:
    description: Slurm Job binary deployment descriptor
    derived_from: yorc.artifacts.Deployment.SlurmJob
  yorc.artifacts.Deployment.SlurmJobImage:
    description: Slurm Job Container image deployment descriptor
    derived_from: yorc.artifacts.Deployment.SlurmJob

node_types:
  yorc.nodes.slurm.Compute:
    derived_from: yorc.nodes.Compute
    properties:
      gres:
        type: string
        required: false
        description: |
          Specifies a comma delimited list of generic consumable resources. The format of each entry on the list is "name[[:type]:count]". The name is that of the consumable resource. The count is the number of those resources with a default value of 1. The specified resources will be allocated to the job on each node. The available generic consumable resources is configurable by the system administrator. Examples of use include "--gres=gpu:2,mic=1", "--gres=gpu:kepler:2", and "--gres=help".
      constraint:
        type: string
        required: false
        description: |
          Nodes can have features assigned to them by the Slurm administrator. Users can specify which of these features are required by their job using the constraint option. Only nodes having features matching the job constraints will be used to satisfy the request. Multiple constraints may be specified with AND, OR, matching OR, resource counts, etc. (some operators are not supported on all system types).
      partition:
        type: string
        required: false
        description: Slurm partition where the nodes will be deployed
      job_name:
        type: string
        required: false
        description: Specify a name for the job allocation. The specified name will appear along with the job id.
    attributes:
      cuda_visible_devices:
        type: string
        description: Coma separated list of visibles GPU devices for the compute.
      job_id:
        type: string
        description: The ID of the job allocation.
      partition:
        type: string
        description: Slurm partition where the nodes are deployed.

  yorc.nodes.slurm.Job:
    derived_from: org.alien4cloud.nodes.Job
    properties:
      name:
        type: string
        description: The slurm job name.
        required: false
      tasks:
        description: Number of tasks to run.
        type: integer
        required: false
        default: 1
      nodes:
        description: Number of nodes allocated to the job.
        type: integer
        required: false
        default: 1
      cpus_per_task:
        description: Number of cpus allocated per task.
        type: integer
        required: false
      mem_per_node:
        type: integer
        description: The memory per node in GB required to the job.
        required: false
      time:
        type: string
        description: >
          Set a limit on the total run time of the job allocation.
          Time formats include "minutes", "minutes:seconds", "hours:minutes:seconds", "days-hours", "days-hours:minutes" and "days-hours:minutes:seconds"
        required: false
      batch:
        type: boolean
        description: True if the job is run on batch mode.
        required: false
        default: true
      extra_options:
        type: list
        description: >
         This define all other slurm job options with the format opt=value (ex: mpi=pmi2 or partition=MyPartition).
        required: false
        entry_schema:
          type: string
      exec_args:
        type: list
        description: >
         This define all arguments passed to the executable run by the job.
        required: false
        entry_schema:
          type: string
      monitoring_time_interval:
        type: string
        description: >
          Time interval duration used for job monitoring as "5s" or "300ms"
          Valid time units are "ns", "us" (or "µs"), "ms", "s", "m", "h".
        required: false
    attributes:
      job_id:
        type: string
        description: The ID of the job.
    interfaces:
      tosca.interfaces.node.lifecycle.Runnable:
        submit:
          implementation:
            # This is a hack to force Alien to generate this step in workflows it will be overrided in Yorc
            # TODO(loicalbertin) think about use a topology modifier for this to add this step only if a submit operation exists
            file: "resources.yaml"
            type: yorc.artifacts.Deployment.SlurmJob
        run:
          implementation:
            # This is a hack to force Alien to generate this step in workflows it will be overrided in Yorc
            # TODO(loicalbertin) think about use a topology modifier for this to add this step only if a submit operation exists
            file: "resources.yaml"
            type: yorc.artifacts.Deployment.SlurmJob
        cancel:
          implementation:
            # This is a hack to force Alien to generate this step in workflows it will be overrided in Yorc
            # TODO(loicalbertin) think about use a topology modifier for this to add this step only if a submit operation exists
            file: "resources.yaml"
            type: yorc.artifacts.Deployment.SlurmJob

  yorc.nodes.slurm.SingularityJob:
    derived_from: yorc.nodes.slurm.Job
    properties:
      exec_command:
        type: string
        description: If this property is filled. The specified command is executed with singularity exec instead of default singularity run.
        required: false